{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd03c80b69efc6c35edf58052dd52e794939bcc5e24ed141d0f365672b2a75355b3",
   "display_name": "Python 3.8.8 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "source": [
    "# Creating functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(\"var%d(t-%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(\"var%d(t)\" % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(\"var%d(t+%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "\n",
    "    # transform data to be stationary\n",
    "    diff_values = raw_values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, verbose=2, shuffle=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq, forecast_len):\n",
    "    forecasts = list()\n",
    "    print(f'Forecast x of {forecast_len}:', end=\" \")\n",
    "    for i in range(forecast_len):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "\n",
    "        # Printing current status in hundreds\n",
    "        step = i % 100\n",
    "        if step == 0:\n",
    "            print(i, end=\" \")\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "\n",
    "        inverted.append(inv_scale)\n",
    "\n",
    "    return inverted"
   ]
  },
  {
   "source": [
    "# Fitting and predicting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing data...\n",
      "Fitting model...\n",
      "Epoch 1/5\n",
      "3356/3356 - 4s - loss: 0.0087\n",
      "Epoch 2/5\n",
      "3356/3356 - 3s - loss: 0.0078\n",
      "Epoch 3/5\n",
      "3356/3356 - 4s - loss: 0.0077\n",
      "Epoch 4/5\n",
      "3356/3356 - 4s - loss: 0.0077\n",
      "Epoch 5/5\n",
      "3356/3356 - 3s - loss: 0.0077\n",
      "Making forecasts...\n",
      "Forecast x of 1000: 0 100 200 300 400 500 600 700 800 900 \n",
      "Inverting forecasts...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "logreturns = \"data/final.csv\"\n",
    "series = pd.read_csv(logreturns, usecols=[\"Exchange.Date\", \"logreturns\"], header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# configure\n",
    "n_lag = 5 # same as ARMA-GARCH\n",
    "n_seq = 63  #  number of periods forecast\n",
    "test_share = 0.25\n",
    "n_test = int(len(series) * test_share)\n",
    "n_epochs = 5\n",
    "n_batch = 1\n",
    "n_neurons = 50\n",
    "forecast_len = 1000\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "print(\"Making forecasts...\")\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq, forecast_len)\n",
    "\n",
    "print(\"\\nInverting forecasts...\")\n",
    "forecasts = inverse_transform(series, forecasts, scaler)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "# Evaluating from t=1\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating dataframe for evaluation\n",
    "In essence, creating a new DF combining training data (historic) and forecasts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dataframe with Close as well and the creating a training df same size as used in the model\n",
    "original_df = pd.read_csv(\"data/final.csv\", usecols=[\"Exchange.Date\", \"logreturns\", \"Close\"])\n",
    "\n",
    "# Setting as date\n",
    "original_df['Exchange.Date'] = original_df['Exchange.Date'].apply(lambda x: date(1900, 1, 1) + timedelta(int(x)))\n",
    "original_df.index = original_df['Exchange.Date']\n",
    "\n",
    "train_df = original_df[:-n_test].copy()\n",
    "\n",
    "# Assigning all rows in train df (before forecast) to closing value\n",
    "# This is because this column cannot be empty (and we have no forecasts since it's training data)\n",
    "train_df[\"forecast\"] = train_df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming logreturns back to price\n",
    "last_train = train_df[\"Close\"].values[-1]\n",
    "price_forecasts = np.exp(np.cumsum(forecasts[0]) + math.log(last_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate dataframe only for forecasts (i.e. \"outside train df\")\n",
    "forecast_df = pd.DataFrame(columns=[\"Exchange.Date\", \"Close\", \"logreturns\", \"forecast\"])\n",
    "forecast_df[\"Close\"] = original_df[\"Close\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"logreturns\"] = original_df[\"logreturns\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"forecast\"] = price_forecasts\n",
    "forecast_df.index\n",
    "\n",
    "forecast_df[\"Exchange.Date\"] = forecast_df.index.map(lambda x: date(2016, 8, 1) + timedelta(int(x)))\n",
    "forecast_df.index = forecast_df[\"Exchange.Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Exchange.Date   Close  logreturns    forecast      error  \\\n",
       "Exchange.Date                                                            \n",
       "2016-09-28       2016-09-28  671.89    0.005567  650.663147 -21.226853   \n",
       "2016-09-29       2016-09-29  671.08   -0.001206  651.891724 -19.188276   \n",
       "2016-09-30       2016-09-30  668.82   -0.003373  653.108582 -15.711418   \n",
       "2016-10-01       2016-10-01  668.02   -0.001197  654.217529 -13.802471   \n",
       "2016-10-02       2016-10-02  663.33   -0.007046  655.444702  -7.885298   \n",
       "\n",
       "               abs_error  actual_up  forecast_up confusion  \n",
       "Exchange.Date                                               \n",
       "2016-09-28     21.226853       True         True        TP  \n",
       "2016-09-29     19.188276      False         True        FP  \n",
       "2016-09-30     15.711418      False         True        FP  \n",
       "2016-10-01     13.802471      False         True        FP  \n",
       "2016-10-02      7.885298      False         True        FP  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Exchange.Date</th>\n      <th>Close</th>\n      <th>logreturns</th>\n      <th>forecast</th>\n      <th>error</th>\n      <th>abs_error</th>\n      <th>actual_up</th>\n      <th>forecast_up</th>\n      <th>confusion</th>\n    </tr>\n    <tr>\n      <th>Exchange.Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-09-28</th>\n      <td>2016-09-28</td>\n      <td>671.89</td>\n      <td>0.005567</td>\n      <td>650.663147</td>\n      <td>-21.226853</td>\n      <td>21.226853</td>\n      <td>True</td>\n      <td>True</td>\n      <td>TP</td>\n    </tr>\n    <tr>\n      <th>2016-09-29</th>\n      <td>2016-09-29</td>\n      <td>671.08</td>\n      <td>-0.001206</td>\n      <td>651.891724</td>\n      <td>-19.188276</td>\n      <td>19.188276</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-09-30</th>\n      <td>2016-09-30</td>\n      <td>668.82</td>\n      <td>-0.003373</td>\n      <td>653.108582</td>\n      <td>-15.711418</td>\n      <td>15.711418</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-10-01</th>\n      <td>2016-10-01</td>\n      <td>668.02</td>\n      <td>-0.001197</td>\n      <td>654.217529</td>\n      <td>-13.802471</td>\n      <td>13.802471</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-10-02</th>\n      <td>2016-10-02</td>\n      <td>663.33</td>\n      <td>-0.007046</td>\n      <td>655.444702</td>\n      <td>-7.885298</td>\n      <td>7.885298</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# Merging train and forecast dataframe\n",
    "merged_df = train_df.append(forecast_df, ignore_index=True)\n",
    "\n",
    "# Creating error, absolute error, actual price going up (True/False) and forecast going up (True/False)\n",
    "merged_df[\"error\"] = merged_df[\"forecast\"] - merged_df[\"Close\"]\n",
    "merged_df[\"abs_error\"] = np.abs(merged_df[\"forecast\"] - merged_df[\"Close\"])\n",
    "merged_df[\"actual_up\"] = merged_df[\"Close\"].diff(1) > 0\n",
    "merged_df[\"forecast_up\"] = merged_df[\"forecast\"].diff(1) > 0\n",
    "merged_df.index = merged_df[\"Exchange.Date\"]\n",
    "\n",
    "# Formula for creating confusion value, used below\n",
    "def confusion(actual, forecast):\n",
    "    if actual and forecast:\n",
    "        return \"TP\"\n",
    "\n",
    "    if actual and not forecast:\n",
    "        return \"FN\"\n",
    "\n",
    "    if not actual and forecast:\n",
    "        return \"FP\"\n",
    "\n",
    "    if not actual and not forecast:\n",
    "        return \"TN\"\n",
    "\n",
    "    # Just common programming sense to return something, could have written \"blabla\"\n",
    "    return False\n",
    "\n",
    "\n",
    "# The lambda stuff applies the above function on every row of data\n",
    "merged_df[\"confusion\"] = merged_df.apply(lambda x: confusion(x[\"actual_up\"], x[\"forecast_up\"]), axis=1)\n",
    "\n",
    "# Printing the tail of the data\n",
    "merged_df.tail()"
   ]
  },
  {
   "source": [
    "## Evaluating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe that only contains the number of periods to evaluate (1,3,5,21,63)\n",
    "def new_df(n_periods):\n",
    "    df = merged_df[len(train_df) : len(train_df) + n_periods]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1, RMSE: 0.709, MAPE: 0.114%\n3, RMSE: 6.076, MAPE: 0.644%\n5, RMSE: 8.219, MAPE: 0.639%\n21, RMSE: 0.511, MAPE: 0.537%\n63, RMSE: 66.282, MAPE: 1.476%\n"
     ]
    }
   ],
   "source": [
    "# Creating RMSE AND MAE\n",
    "def evaluate(n_periods):\n",
    "    df = new_df(n_periods)\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    print(f\"{n_periods}, RMSE: {round(rmse, 3)}, MAPE: {round(mape, 3)}%\")\n",
    "\n",
    "\n",
    "evaluate(1)  # 1 day\n",
    "evaluate(3)  # half a week\n",
    "evaluate(5)  # week\n",
    "evaluate(21)  # month\n",
    "evaluate(63)  # quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    P  N\nP  31  6\nN  25  1\nprecision: 83%, recall: 55%, f-score: 0.667\n"
     ]
    }
   ],
   "source": [
    "# Creating confusion matrix\n",
    "def confusion_matrix(df):\n",
    "    conf = pd.DataFrame(columns=[\"P\", \"N\"], index=[\"P\", \"N\"])\n",
    "    conf.loc[\"P\", \"P\"] = len(df[df[\"confusion\"] == \"TP\"])\n",
    "    conf.loc[\"P\", \"N\"] = len(df[df[\"confusion\"] == \"FN\"])\n",
    "    conf.loc[\"N\", \"P\"] = len(df[df[\"confusion\"] == \"FP\"])\n",
    "    conf.loc[\"N\", \"N\"] = len(df[df[\"confusion\"] == \"TN\"])\n",
    "    return conf\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(new_df(63))\n",
    "precision = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[1, 0])\n",
    "recall = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[0, 1])\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(confusion)\n",
    "print(f\"precision: {int(precision*100)}%, recall: {int(recall*100)}%, f-score: {round(f_score, 3)}\")"
   ]
  },
  {
   "source": [
    "# Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = merged_df[-n_seq - (n_seq * 2) :]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(plot_df[\"forecast\"], label=\"forecast\")\n",
    "plt.plot(plot_df[\"Close\"], label=\"actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "# Cross-validating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fac95e6e8f12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrmse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m cross_df = pd.DataFrame(columns=[\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"mape_1\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;34m\"mape_3\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "def cross_evaluate(df, n_periods):\n",
    "    df = df[-63:-63+n_periods] if n_periods < 63 else df.tail(63)\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    return mape, rmse\n",
    "\n",
    "cross_df = pd.DataFrame(columns=[\n",
    "    \"mape_1\", \n",
    "    \"mape_3\",\n",
    "    \"mape_5\",\n",
    "    \"mape_21\",\n",
    "    \"mape_63\",\n",
    "    \"rmse_1\",\n",
    "    \"rmse_3\",\n",
    "    \"rmse_5\",\n",
    "    \"rmse_21\",\n",
    "    \"rmse_63\"\n",
    "])\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    train_df = original_df[:-n_test + i].copy()\n",
    "    train_df[\"forecast\"] = train_df[\"Close\"]\n",
    "\n",
    "    last_train = train_df[\"Close\"].values[-1]\n",
    "    price_forecasts = np.exp(np.cumsum(forecasts[i]) + math.log(last_train))\n",
    "\n",
    "    cross_merged_df = original_df[-n_test + i:-n_test + i + 63].copy()\n",
    "    cross_merged_df[\"forecast\"] = price_forecasts\n",
    "\n",
    "    cross_merged_df[\"error\"] = cross_merged_df[\"forecast\"] - cross_merged_df[\"Close\"]\n",
    "    cross_merged_df[\"abs_error\"] = np.abs(cross_merged_df[\"forecast\"] - cross_merged_df[\"Close\"])\n",
    "\n",
    "    one = cross_evaluate(cross_merged_df, 1)\n",
    "    three = cross_evaluate(cross_merged_df, 3)\n",
    "    five = cross_evaluate(cross_merged_df, 5)\n",
    "    twentyone = cross_evaluate(cross_merged_df, 21)\n",
    "    sixtythree = cross_evaluate(cross_merged_df, 63)\n",
    "\n",
    "    cross_df = cross_df.append({\n",
    "        'mape_1': one[0],\n",
    "        'mape_3': three[0],\n",
    "        'mape_5': five[0],\n",
    "        'mape_21': twentyone[0],\n",
    "        'mape_63': sixtythree[0],\n",
    "        'rmse_1': one[1],\n",
    "        'rmse_3': three[1],\n",
    "        'rmse_5': five[1],\n",
    "        'rmse_21': twentyone[1],\n",
    "        'rmse_63': sixtythree[1],\n",
    "    }, ignore_index=True)\n",
    "\n",
    "cross_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            mape_1       mape_3       mape_5      mape_21      mape_63  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.619687     0.943839     1.222891     2.815316     5.451576   \n",
       "std       0.819616     1.102143     1.464113     3.286426     4.312766   \n",
       "min       0.001937     0.036980     0.126373     0.329151     0.751996   \n",
       "25%       0.188671     0.370524     0.495427     1.263913     2.625553   \n",
       "50%       0.428625     0.642222     0.836517     2.024687     4.258856   \n",
       "75%       0.753446     1.138278     1.440175     3.052294     6.573215   \n",
       "max      14.052270    15.741533    18.920082    27.296362    27.491920   \n",
       "\n",
       "            rmse_1       rmse_3       rmse_5      rmse_21      rmse_63  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean      5.021035    12.395882    20.414207    95.029710   325.260300  \n",
       "std       6.365685    15.223010    25.937659   116.688066   293.582137  \n",
       "min       0.015474     0.029424     0.003182     0.332651     0.353790  \n",
       "25%       1.525070     3.890628     6.210660    30.827729   125.210458  \n",
       "50%       3.394917     8.147912    13.840070    67.895755   252.833447  \n",
       "75%       6.204407    15.495337    25.677703   110.530774   418.426647  \n",
       "max      99.719121   189.141600   285.228183   919.271311  2018.404728  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mape_1</th>\n      <th>mape_3</th>\n      <th>mape_5</th>\n      <th>mape_21</th>\n      <th>mape_63</th>\n      <th>rmse_1</th>\n      <th>rmse_3</th>\n      <th>rmse_5</th>\n      <th>rmse_21</th>\n      <th>rmse_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.619687</td>\n      <td>0.943839</td>\n      <td>1.222891</td>\n      <td>2.815316</td>\n      <td>5.451576</td>\n      <td>5.021035</td>\n      <td>12.395882</td>\n      <td>20.414207</td>\n      <td>95.029710</td>\n      <td>325.260300</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.819616</td>\n      <td>1.102143</td>\n      <td>1.464113</td>\n      <td>3.286426</td>\n      <td>4.312766</td>\n      <td>6.365685</td>\n      <td>15.223010</td>\n      <td>25.937659</td>\n      <td>116.688066</td>\n      <td>293.582137</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001937</td>\n      <td>0.036980</td>\n      <td>0.126373</td>\n      <td>0.329151</td>\n      <td>0.751996</td>\n      <td>0.015474</td>\n      <td>0.029424</td>\n      <td>0.003182</td>\n      <td>0.332651</td>\n      <td>0.353790</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.188671</td>\n      <td>0.370524</td>\n      <td>0.495427</td>\n      <td>1.263913</td>\n      <td>2.625553</td>\n      <td>1.525070</td>\n      <td>3.890628</td>\n      <td>6.210660</td>\n      <td>30.827729</td>\n      <td>125.210458</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.428625</td>\n      <td>0.642222</td>\n      <td>0.836517</td>\n      <td>2.024687</td>\n      <td>4.258856</td>\n      <td>3.394917</td>\n      <td>8.147912</td>\n      <td>13.840070</td>\n      <td>67.895755</td>\n      <td>252.833447</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.753446</td>\n      <td>1.138278</td>\n      <td>1.440175</td>\n      <td>3.052294</td>\n      <td>6.573215</td>\n      <td>6.204407</td>\n      <td>15.495337</td>\n      <td>25.677703</td>\n      <td>110.530774</td>\n      <td>418.426647</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>14.052270</td>\n      <td>15.741533</td>\n      <td>18.920082</td>\n      <td>27.296362</td>\n      <td>27.491920</td>\n      <td>99.719121</td>\n      <td>189.141600</td>\n      <td>285.228183</td>\n      <td>919.271311</td>\n      <td>2018.404728</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "cross_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   measure        mean       lower       upper\n",
       "0   mape_1    0.619687    0.568886    0.670487\n",
       "1   mape_3    0.943839    0.875527    1.012150\n",
       "2   mape_5    1.222891    1.132144    1.313638\n",
       "3  mape_21    2.815316    2.611621    3.019011\n",
       "4  mape_63    5.451576    5.184268    5.718884\n",
       "5   rmse_1    5.021035    4.626486    5.415585\n",
       "6   rmse_3   12.395882   11.452350   13.339414\n",
       "7   rmse_5   20.414207   18.806575   22.021840\n",
       "8  rmse_21   95.029710   87.797308  102.262111\n",
       "9  rmse_63  325.260300  307.063891  343.456710"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>mean</th>\n      <th>lower</th>\n      <th>upper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mape_1</td>\n      <td>0.619687</td>\n      <td>0.568886</td>\n      <td>0.670487</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mape_3</td>\n      <td>0.943839</td>\n      <td>0.875527</td>\n      <td>1.012150</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mape_5</td>\n      <td>1.222891</td>\n      <td>1.132144</td>\n      <td>1.313638</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mape_21</td>\n      <td>2.815316</td>\n      <td>2.611621</td>\n      <td>3.019011</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mape_63</td>\n      <td>5.451576</td>\n      <td>5.184268</td>\n      <td>5.718884</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>rmse_1</td>\n      <td>5.021035</td>\n      <td>4.626486</td>\n      <td>5.415585</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>rmse_3</td>\n      <td>12.395882</td>\n      <td>11.452350</td>\n      <td>13.339414</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>rmse_5</td>\n      <td>20.414207</td>\n      <td>18.806575</td>\n      <td>22.021840</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rmse_21</td>\n      <td>95.029710</td>\n      <td>87.797308</td>\n      <td>102.262111</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>rmse_63</td>\n      <td>325.260300</td>\n      <td>307.063891</td>\n      <td>343.456710</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "n = cross_df.count()[0]\n",
    "mean = cross_df.mean()\n",
    "upper = cross_df.mean() + 1.96 * cross_df.std() / math.sqrt(n)\n",
    "lower = cross_df.mean() - 1.96 * cross_df.std() / math.sqrt(n)\n",
    "\n",
    "ci_df = pd.DataFrame(columns=['measure', 'mean', 'lower', 'upper'])\n",
    "\n",
    "for i in range(10):\n",
    "    ci_df = ci_df.append({\n",
    "        'measure': cross_df.columns[i],\n",
    "        'mean': mean[i],\n",
    "        'lower': lower[i],\n",
    "        'upper': upper[i]\n",
    "    }, ignore_index=True)\n",
    "\n",
    "ci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}