{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "3c80b69efc6c35edf58052dd52e794939bcc5e24ed141d0f365672b2a75355b3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "source": [
    "# Creating functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(\"var%d(t-%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(\"var%d(t)\" % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(\"var%d(t+%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "\n",
    "    # transform data to be stationary\n",
    "    diff_values = raw_values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, verbose=2, shuffle=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    print(f'Forecast x of {len(test)}:', end=\" \")\n",
    "    for i in range(len(test)):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "\n",
    "        # Printing current status in hundreds\n",
    "        hundred = i % 100\n",
    "        if hundred == 0:\n",
    "            print(i, end=\" \")\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "\n",
    "        inverted.append(inv_scale)\n",
    "\n",
    "    return inverted"
   ]
  },
  {
   "source": [
    "# Fitting and predicting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "logreturns = \"data/final.csv\"\n",
    "series = pd.read_csv(logreturns, usecols=[\"Exchange.Date\", \"logreturns\"], header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# configure\n",
    "n_lag = 63\n",
    "n_seq = 63  #  number of periods forecast\n",
    "test_share = 0.25\n",
    "n_test = int(len(series) * test_share)\n",
    "n_epochs = 5\n",
    "n_batch = 1\n",
    "n_neurons = 50\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "print(\"Making forecasts...\")\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "\n",
    "print(\"\\nInverting forecasts...\")\n",
    "forecasts = inverse_transform(series, forecasts, scaler)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "# Evaluating from t=1\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating dataframe for evaluation\n",
    "In essence, creating a new DF combining training data (historic) and forecasts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dataframe with Close as well and the creating a training df same size as used in the model\n",
    "original_df = pd.read_csv(\"data/final.csv\", usecols=[\"logreturns\", \"Close\"])\n",
    "train_df = original_df[:-n_test].copy()\n",
    "\n",
    "# Assigning all rows in train df (before forecast) to closing value\n",
    "# This is because this column cannot be empty (and we have no forecasts since it's training data)\n",
    "train_df[\"forecast\"] = train_df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming logreturns back to price\n",
    "last_train = train_df[\"Close\"].values[-1]\n",
    "price_forecasts = np.exp(np.cumsum(forecasts[0]) + math.log(last_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate dataframe only for forecasts (i.e. \"outside train df\")\n",
    "forecast_df = pd.DataFrame(columns=[\"Close\", \"logreturns\", \"forecast\"])\n",
    "forecast_df[\"Close\"] = original_df[\"Close\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"logreturns\"] = original_df[\"logreturns\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"forecast\"] = price_forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging train and forecast dataframe\n",
    "merged_df = train_df.append(forecast_df, ignore_index=True)\n",
    "\n",
    "# Creating error, absolute error, actual price going up (True/False) and forecast going up (True/False)\n",
    "merged_df[\"error\"] = merged_df[\"forecast\"] - merged_df[\"Close\"]\n",
    "merged_df[\"abs_error\"] = np.abs(merged_df[\"forecast\"] - merged_df[\"Close\"])\n",
    "merged_df[\"actual_up\"] = merged_df[\"Close\"].diff(1) > 0\n",
    "merged_df[\"forecast_up\"] = merged_df[\"forecast\"].diff(1) > 0\n",
    "\n",
    "# Formula for creating confusion value, used below\n",
    "def confusion(actual, forecast):\n",
    "    if actual and forecast:\n",
    "        return \"TP\"\n",
    "\n",
    "    if actual and not forecast:\n",
    "        return \"FN\"\n",
    "\n",
    "    if not actual and forecast:\n",
    "        return \"FP\"\n",
    "\n",
    "    if not actual and not forecast:\n",
    "        return \"TN\"\n",
    "\n",
    "    # Just common programming sense to return something, could have written \"blabla\"\n",
    "    return False\n",
    "\n",
    "\n",
    "# The lambda stuff applies the above function on every row of data\n",
    "merged_df[\"confusion\"] = merged_df.apply(lambda x: confusion(x[\"actual_up\"], x[\"forecast_up\"]), axis=1)\n",
    "\n",
    "# Printing the tail of the data\n",
    "merged_df.tail()"
   ]
  },
  {
   "source": [
    "## Evaluating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe that only contains the number of periods to evaluate (1,3,5,21,63)\n",
    "def new_df(n_periods):\n",
    "    df = merged_df[len(train_df) : len(train_df) + n_periods]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating RMSE AND MAE\n",
    "def evaluate(n_periods):\n",
    "    df = new_df(n_periods)\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    print(f\"{n_periods}, RMSE: {round(rmse, 3)}, MAPE: {round(mape, 3)}%\")\n",
    "\n",
    "\n",
    "evaluate(1)  # 1 day\n",
    "evaluate(3)  # half a week\n",
    "evaluate(5)  # week\n",
    "evaluate(21)  # month\n",
    "evaluate(63)  # quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "def confusion_matrix(df):\n",
    "    conf = pd.DataFrame(columns=[\"P\", \"N\"], index=[\"P\", \"N\"])\n",
    "    conf.loc[\"P\", \"P\"] = len(df[df[\"confusion\"] == \"TP\"])\n",
    "    conf.loc[\"P\", \"N\"] = len(df[df[\"confusion\"] == \"FN\"])\n",
    "    conf.loc[\"N\", \"P\"] = len(df[df[\"confusion\"] == \"FP\"])\n",
    "    conf.loc[\"N\", \"N\"] = len(df[df[\"confusion\"] == \"TN\"])\n",
    "    return conf\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(new_df(63))\n",
    "precision = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[0, 1])\n",
    "recall = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[1, 0])\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(confusion)\n",
    "print(f\"precision: {int(precision*100)}%, recall: {int(recall*100)}%, f-score: {round(f_score, 3)}\")"
   ]
  },
  {
   "source": [
    "# Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = merged_df[-n_seq - (n_seq * 2) :]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(plot_df[\"forecast\"], label=\"forecast\")\n",
    "plt.plot(plot_df[\"Close\"], label=\"actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}