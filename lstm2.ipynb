{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd03c80b69efc6c35edf58052dd52e794939bcc5e24ed141d0f365672b2a75355b3",
   "display_name": "Python 3.8.8 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "source": [
    "# Creating functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(\"var%d(t-%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(\"var%d(t)\" % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(\"var%d(t+%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "\n",
    "    # transform data to be stationary\n",
    "    diff_values = raw_values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, verbose=2, shuffle=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq, forecast_len):\n",
    "    forecasts = list()\n",
    "    print(f'Forecast x of {forecast_len}:', end=\" \")\n",
    "    for i in range(forecast_len):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "\n",
    "        # Printing current status in hundreds\n",
    "        step = i % 100\n",
    "        if step == 0:\n",
    "            print(i, end=\" \")\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "\n",
    "        inverted.append(inv_scale)\n",
    "\n",
    "    return inverted"
   ]
  },
  {
   "source": [
    "# Fitting and predicting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing data...\n",
      "Fitting model...\n",
      "Epoch 1/5\n",
      "3356/3356 - 6s - loss: 0.0086\n",
      "Epoch 2/5\n",
      "3356/3356 - 6s - loss: 0.0078\n",
      "Epoch 3/5\n",
      "3356/3356 - 4s - loss: 0.0077\n",
      "Epoch 4/5\n",
      "3356/3356 - 4s - loss: 0.0077\n",
      "Epoch 5/5\n",
      "3356/3356 - 4s - loss: 0.0077\n",
      "Making forecasts...\n",
      "Forecast x of 1000: 0 100 200 300 400 500 600 700 800 900 \n",
      "Inverting forecasts...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "logreturns = \"data/final.csv\"\n",
    "series = pd.read_csv(logreturns, usecols=[\"Exchange.Date\", \"logreturns\"], header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# configure\n",
    "n_lag = 5 # same as ARMA-GARCH\n",
    "n_seq = 63  #  number of periods forecast\n",
    "test_share = 0.25\n",
    "n_test = int(len(series) * test_share)\n",
    "n_epochs = 5\n",
    "n_batch = 1\n",
    "n_neurons = 50\n",
    "forecast_len = 1000\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "print(\"Making forecasts...\")\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq, forecast_len)\n",
    "\n",
    "print(\"\\nInverting forecasts...\")\n",
    "forecasts = inverse_transform(series, forecasts, scaler)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "# Evaluating from t=1\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating dataframe for evaluation\n",
    "In essence, creating a new DF combining training data (historic) and forecasts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dataframe with Close as well and the creating a training df same size as used in the model\n",
    "original_df = pd.read_csv(\"data/final.csv\", usecols=[\"Exchange.Date\", \"logreturns\", \"Close\"])\n",
    "\n",
    "# Setting as date\n",
    "original_df['Exchange.Date'] = original_df['Exchange.Date'].apply(lambda x: date(1900, 1, 1) + timedelta(int(x)))\n",
    "original_df.index = original_df['Exchange.Date']\n",
    "\n",
    "train_df = original_df[:-n_test].copy()\n",
    "\n",
    "# Assigning all rows in train df (before forecast) to closing value\n",
    "# This is because this column cannot be empty (and we have no forecasts since it's training data)\n",
    "train_df[\"forecast\"] = train_df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming logreturns back to price\n",
    "last_train = train_df[\"Close\"].values[-1]\n",
    "price_forecasts = np.exp(np.cumsum(forecasts[0]) + math.log(last_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate dataframe only for forecasts (i.e. \"outside train df\")\n",
    "forecast_df = pd.DataFrame(columns=[\"Exchange.Date\", \"Close\", \"logreturns\", \"forecast\"])\n",
    "forecast_df[\"Close\"] = original_df[\"Close\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"logreturns\"] = original_df[\"logreturns\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"forecast\"] = price_forecasts\n",
    "forecast_df.index\n",
    "\n",
    "forecast_df[\"Exchange.Date\"] = forecast_df.index.map(lambda x: date(2016, 8, 1) + timedelta(int(x)))\n",
    "forecast_df.index = forecast_df[\"Exchange.Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Exchange.Date   Close  logreturns    forecast      error  \\\n",
       "Exchange.Date                                                            \n",
       "2016-09-28       2016-09-28  671.89    0.005567  655.097839 -16.792161   \n",
       "2016-09-29       2016-09-29  671.08   -0.001206  656.174255 -14.905745   \n",
       "2016-09-30       2016-09-30  668.82   -0.003373  657.349854 -11.470146   \n",
       "2016-10-01       2016-10-01  668.02   -0.001197  658.540466  -9.479534   \n",
       "2016-10-02       2016-10-02  663.33   -0.007046  659.892822  -3.437178   \n",
       "\n",
       "               abs_error  actual_up  forecast_up confusion  \n",
       "Exchange.Date                                               \n",
       "2016-09-28     16.792161       True         True        TP  \n",
       "2016-09-29     14.905745      False         True        FP  \n",
       "2016-09-30     11.470146      False         True        FP  \n",
       "2016-10-01      9.479534      False         True        FP  \n",
       "2016-10-02      3.437178      False         True        FP  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Exchange.Date</th>\n      <th>Close</th>\n      <th>logreturns</th>\n      <th>forecast</th>\n      <th>error</th>\n      <th>abs_error</th>\n      <th>actual_up</th>\n      <th>forecast_up</th>\n      <th>confusion</th>\n    </tr>\n    <tr>\n      <th>Exchange.Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-09-28</th>\n      <td>2016-09-28</td>\n      <td>671.89</td>\n      <td>0.005567</td>\n      <td>655.097839</td>\n      <td>-16.792161</td>\n      <td>16.792161</td>\n      <td>True</td>\n      <td>True</td>\n      <td>TP</td>\n    </tr>\n    <tr>\n      <th>2016-09-29</th>\n      <td>2016-09-29</td>\n      <td>671.08</td>\n      <td>-0.001206</td>\n      <td>656.174255</td>\n      <td>-14.905745</td>\n      <td>14.905745</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-09-30</th>\n      <td>2016-09-30</td>\n      <td>668.82</td>\n      <td>-0.003373</td>\n      <td>657.349854</td>\n      <td>-11.470146</td>\n      <td>11.470146</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-10-01</th>\n      <td>2016-10-01</td>\n      <td>668.02</td>\n      <td>-0.001197</td>\n      <td>658.540466</td>\n      <td>-9.479534</td>\n      <td>9.479534</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-10-02</th>\n      <td>2016-10-02</td>\n      <td>663.33</td>\n      <td>-0.007046</td>\n      <td>659.892822</td>\n      <td>-3.437178</td>\n      <td>3.437178</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Merging train and forecast dataframe\n",
    "merged_df = train_df.append(forecast_df, ignore_index=True)\n",
    "\n",
    "# Creating error, absolute error, actual price going up (True/False) and forecast going up (True/False)\n",
    "merged_df[\"error\"] = merged_df[\"forecast\"] - merged_df[\"Close\"]\n",
    "merged_df[\"abs_error\"] = np.abs(merged_df[\"forecast\"] - merged_df[\"Close\"])\n",
    "merged_df[\"actual_up\"] = merged_df[\"Close\"].diff(1) > 0\n",
    "merged_df[\"forecast_up\"] = merged_df[\"forecast\"].diff(1) > 0\n",
    "merged_df.index = merged_df[\"Exchange.Date\"]\n",
    "\n",
    "# Formula for creating confusion value, used below\n",
    "def confusion(actual, forecast):\n",
    "    if actual and forecast:\n",
    "        return \"TP\"\n",
    "\n",
    "    if actual and not forecast:\n",
    "        return \"FN\"\n",
    "\n",
    "    if not actual and forecast:\n",
    "        return \"FP\"\n",
    "\n",
    "    if not actual and not forecast:\n",
    "        return \"TN\"\n",
    "\n",
    "    # Just common programming sense to return something, could have written \"blabla\"\n",
    "    return False\n",
    "\n",
    "\n",
    "# The lambda stuff applies the above function on every row of data\n",
    "merged_df[\"confusion\"] = merged_df.apply(lambda x: confusion(x[\"actual_up\"], x[\"forecast_up\"]), axis=1)\n",
    "\n",
    "# Printing the tail of the data\n",
    "merged_df.tail()"
   ]
  },
  {
   "source": [
    "## Evaluating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe that only contains the number of periods to evaluate (1,3,5,21,63)\n",
    "def new_df(n_periods):\n",
    "    df = merged_df[len(train_df) : len(train_df) + n_periods]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1, RMSE: 0.648, MAPE: 0.104%\n3, RMSE: 6.319, MAPE: 0.66%\n5, RMSE: 8.584, MAPE: 0.661%\n21, RMSE: 5.722, MAPE: 0.478%\n63, RMSE: 44.001, MAPE: 1.174%\n"
     ]
    }
   ],
   "source": [
    "# Creating RMSE AND MAE\n",
    "def evaluate(n_periods):\n",
    "    df = new_df(n_periods)\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    print(f\"{n_periods}, RMSE: {round(rmse, 3)}, MAPE: {round(mape, 3)}%\")\n",
    "\n",
    "\n",
    "evaluate(1)  # 1 day\n",
    "evaluate(3)  # half a week\n",
    "evaluate(5)  # week\n",
    "evaluate(21)  # month\n",
    "evaluate(63)  # quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   P  N\nP  1  0\nN  2  0\nprecision: 33%, recall: 100%, f-score: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Creating confusion matrix\n",
    "def confusion_matrix(df):\n",
    "    conf = pd.DataFrame(columns=[\"P\", \"N\"], index=[\"P\", \"N\"])\n",
    "    conf.loc[\"P\", \"P\"] = len(df[df[\"confusion\"] == \"TP\"])\n",
    "    conf.loc[\"P\", \"N\"] = len(df[df[\"confusion\"] == \"FN\"])\n",
    "    conf.loc[\"N\", \"P\"] = len(df[df[\"confusion\"] == \"FP\"])\n",
    "    conf.loc[\"N\", \"N\"] = len(df[df[\"confusion\"] == \"TN\"])\n",
    "    return conf\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(new_df(3))\n",
    "precision = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[1, 0])\n",
    "recall = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[0, 1])\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(confusion)\n",
    "print(f\"precision: {int(precision*100)}%, recall: {int(recall*100)}%, f-score: {round(f_score, 3)}\")"
   ]
  },
  {
   "source": [
    "# Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = merged_df[-n_seq - (n_seq * 2) :]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(plot_df[\"forecast\"], label=\"forecast\")\n",
    "plt.plot(plot_df[\"Close\"], label=\"actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "# Cross-validating 2.0\n",
    "The above one is messy, this is (hopefully) better. And more like arch_evaluate"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Generating CSV\n",
    "_Not recommended to run this chunk, use the one below (this one takes ~ 5 minutes to finish, since it's creating the csv)_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 100 200 300 400 500 600 700 800 900 done!\n"
     ]
    }
   ],
   "source": [
    "# Creating df that is as alike arch_evaluate as possible\n",
    "original_df = pd.read_csv(\"data/final.csv\", usecols=[\"Close\"])\n",
    "cross_df = pd.DataFrame(columns=['time', 'forecast', 'Close'])\n",
    "\n",
    "forecast_len = 1000\n",
    "for i in range(forecast_len):\n",
    "    train_size_cv = int(len(original_df) * 0.75) + i\n",
    "    train_cv = original_df[:train_size_cv]\n",
    "    last_train = train_cv.values[-1]\n",
    "    test_cv = original_df[train_size_cv : len(original_df)]\n",
    "\n",
    "    price_forecasts = np.exp(np.cumsum(forecasts[i]) + math.log(last_train))\n",
    "\n",
    "    for j, forecast in enumerate(price_forecasts):\n",
    "        cross_df = cross_df.append({\n",
    "            'time': i+1,\n",
    "            'forecast': forecast,\n",
    "            'Close': test_cv['Close'].values[j]\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    step = i % 100\n",
    "    if step == 0:\n",
    "        print(i, end=\" \")\n",
    "\n",
    "print('done!')\n",
    "cross_df.to_csv('data/lstm_cross_val.csv', index=False)"
   ]
  },
  {
   "source": [
    "## Reading CSV and assigning columns\n",
    "_Run this chunk instead, it reads the csv from the chunk above_"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   time   Close    forecast     error  abs_error  actual_up  forecast_up  \\\n",
       "0   0.0  621.38  621.380000  0.000000   0.000000      False        False   \n",
       "1   1.0  622.77  622.121887 -0.648113   0.648113       True         True   \n",
       "2   1.0  618.70  623.105164  4.405164   4.405164      False         True   \n",
       "3   1.0  617.12  624.307251  7.187251   7.187251      False         True   \n",
       "4   1.0  621.28  625.417114  4.137114   4.137114       True         True   \n",
       "\n",
       "  confusion  \n",
       "0        TN  \n",
       "1        TP  \n",
       "2        FP  \n",
       "3        FP  \n",
       "4        TP  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>Close</th>\n      <th>forecast</th>\n      <th>error</th>\n      <th>abs_error</th>\n      <th>actual_up</th>\n      <th>forecast_up</th>\n      <th>confusion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>621.38</td>\n      <td>621.380000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>False</td>\n      <td>TN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>622.77</td>\n      <td>622.121887</td>\n      <td>-0.648113</td>\n      <td>0.648113</td>\n      <td>True</td>\n      <td>True</td>\n      <td>TP</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>618.70</td>\n      <td>623.105164</td>\n      <td>4.405164</td>\n      <td>4.405164</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.0</td>\n      <td>617.12</td>\n      <td>624.307251</td>\n      <td>7.187251</td>\n      <td>7.187251</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>621.28</td>\n      <td>625.417114</td>\n      <td>4.137114</td>\n      <td>4.137114</td>\n      <td>True</td>\n      <td>True</td>\n      <td>TP</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "# Creating dataframe columns (error, absolute error, actual_up, forecast_up och confusion (TP, FP, TN, FN))\n",
    "lstm_cross_df = pd.read_csv('data/lstm_cross_val.csv')\n",
    "\n",
    "# adding first row of data based on last row of test data\n",
    "new_data = []\n",
    "new_data.insert(0, {'time':0, 'Close': 621.38, 'forecast': 621.38})\n",
    "lstm_cross_df = pd.concat([pd.DataFrame(new_data), lstm_cross_df], ignore_index=True)\n",
    "\n",
    "# creating error and up columns \n",
    "lstm_cross_df['error'] = lstm_cross_df['forecast'] - lstm_cross_df['Close']\n",
    "lstm_cross_df['abs_error'] = np.abs(lstm_cross_df['forecast'] - lstm_cross_df['Close'])\n",
    "lstm_cross_df['actual_up'] = lstm_cross_df['Close'].diff(1) > 0\n",
    "lstm_cross_df['forecast_up'] = lstm_cross_df['forecast'].diff(1) > 0\n",
    "\n",
    "def confusion(actual, forecast):\n",
    "    if (actual and forecast):\n",
    "        return 'TP'\n",
    "    \n",
    "    if (actual and not forecast):\n",
    "        return 'FN'\n",
    "    \n",
    "    if (not actual and forecast):\n",
    "        return 'FP'\n",
    "    \n",
    "    if (not actual and not forecast):\n",
    "        return 'TN'\n",
    "    \n",
    "    return False\n",
    "\n",
    "lstm_cross_df['confusion'] = lstm_cross_df.apply(lambda x: confusion(x['actual_up'], x['forecast_up']), axis=1)\n",
    "\n",
    "lstm_cross_df.head(5)"
   ]
  },
  {
   "source": [
    "## Creating cross evaluated columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     mape_1    mape_3   mape_5   mape_21  mape_63    rmse_1    rmse_3  \\\n",
       "0  0.104069  0.660239  0.66148  0.477608  1.17385  0.648113  6.318696   \n",
       "\n",
       "    rmse_5   rmse_21    rmse_63  ...  recall_1  recall_3  recall_5  recall_21  \\\n",
       "0  8.58361  5.722093  44.000889  ...       1.0       1.0       1.0        1.0   \n",
       "\n",
       "   recall_63  fscore_1  fscore_3  fscore_5  fscore_21  fscore_63  \n",
       "0   0.972973       1.0       0.5      0.75   0.764706   0.727273  \n",
       "\n",
       "[1 rows x 25 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mape_1</th>\n      <th>mape_3</th>\n      <th>mape_5</th>\n      <th>mape_21</th>\n      <th>mape_63</th>\n      <th>rmse_1</th>\n      <th>rmse_3</th>\n      <th>rmse_5</th>\n      <th>rmse_21</th>\n      <th>rmse_63</th>\n      <th>...</th>\n      <th>recall_1</th>\n      <th>recall_3</th>\n      <th>recall_5</th>\n      <th>recall_21</th>\n      <th>recall_63</th>\n      <th>fscore_1</th>\n      <th>fscore_3</th>\n      <th>fscore_5</th>\n      <th>fscore_21</th>\n      <th>fscore_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.104069</td>\n      <td>0.660239</td>\n      <td>0.66148</td>\n      <td>0.477608</td>\n      <td>1.17385</td>\n      <td>0.648113</td>\n      <td>6.318696</td>\n      <td>8.58361</td>\n      <td>5.722093</td>\n      <td>44.000889</td>\n      <td>...</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.972973</td>\n      <td>1.0</td>\n      <td>0.5</td>\n      <td>0.75</td>\n      <td>0.764706</td>\n      <td>0.727273</td>\n    </tr>\n  </tbody>\n</table>\n<p>1 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "def cross_evaluate(df, n_periods):\n",
    "    df = df[-63:-63+n_periods] if n_periods < 63 else df.tail(63)\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "\n",
    "    tp = len(df[df['confusion'] == 'TP'])\n",
    "    fp = len(df[df['confusion'] == 'FP'])\n",
    "    fn = len(df[df['confusion'] == 'FN'])\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0 # if else för att undvika division by zero errror\n",
    "    recall = tp / (tp + fn) if (tp + fn > 0) else 0\n",
    "    fscore = (2*precision*recall)/(precision+recall) if (precision + recall > 0) else 0\n",
    "\n",
    "    return mape, rmse, precision, recall, fscore\n",
    "\n",
    "cross_df = pd.DataFrame(columns=[\n",
    "    \"mape_1\", \n",
    "    \"mape_3\",\n",
    "    \"mape_5\",\n",
    "    \"mape_21\",\n",
    "    \"mape_63\",\n",
    "    \"rmse_1\",\n",
    "    \"rmse_3\",\n",
    "    \"rmse_5\",\n",
    "    \"rmse_21\",\n",
    "    \"rmse_63\",\n",
    "    'precision_1',\n",
    "    'precision_3',\n",
    "    'precision_5',\n",
    "    'precision_21',\n",
    "    'precision_63',\n",
    "    'recall_1',\n",
    "    'recall_3',\n",
    "    'recall_5',\n",
    "    'recall_21',\n",
    "    'recall_63',\n",
    "    'fscore_1',\n",
    "    'fscore_3',\n",
    "    'fscore_5',\n",
    "    'fscore_21',\n",
    "    'fscore_63',\n",
    "])\n",
    "\n",
    "forecast_len = 1000\n",
    "for i in range(forecast_len):\n",
    "    cross_merged_df = lstm_cross_df[i+1 : i+63+1] # to avoid first row 1 is added (which contains last training)\n",
    "    one = cross_evaluate(cross_merged_df, 1)\n",
    "    three = cross_evaluate(cross_merged_df, 3)\n",
    "    five = cross_evaluate(cross_merged_df, 5)\n",
    "    twentyone = cross_evaluate(cross_merged_df, 21)\n",
    "    sixtythree = cross_evaluate(cross_merged_df, 63)\n",
    "\n",
    "    cross_df = cross_df.append({\n",
    "        'mape_1': one[0],\n",
    "        'mape_3': three[0],\n",
    "        'mape_5': five[0],\n",
    "        'mape_21': twentyone[0],\n",
    "        'mape_63': sixtythree[0],\n",
    "        'rmse_1': one[1],\n",
    "        'rmse_3': three[1],\n",
    "        'rmse_5': five[1],\n",
    "        'rmse_21': twentyone[1],\n",
    "        'rmse_63': sixtythree[1],\n",
    "        'precision_1': one[2],\n",
    "        'precision_3': three[2],\n",
    "        'precision_5': five[2],\n",
    "        'precision_21': twentyone[2],\n",
    "        'precision_63': sixtythree[2],\n",
    "        'recall_1': one[3],\n",
    "        'recall_3': three[3],\n",
    "        'recall_5': five[3],\n",
    "        'recall_21': twentyone[3],\n",
    "        'recall_63': sixtythree[3],\n",
    "        'fscore_1': one[4],\n",
    "        'fscore_3': three[4],\n",
    "        'fscore_5': five[4],\n",
    "        'fscore_21': twentyone[4],\n",
    "        'fscore_63': sixtythree[4],\n",
    "    }, ignore_index=True)\n",
    "\n",
    "cross_df.head(1) # notera hur raden här är identisk med resultatet när vi inte körde korsvalidering"
   ]
  },
  {
   "source": [
    "## Description of RMSE and MAPE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            mape_1       mape_3       mape_5      mape_21      mape_63  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      2.658915     2.664283     2.669997     2.694996     2.713831   \n",
       "std       1.975212     1.917600     1.871873     1.570899     0.920836   \n",
       "min       0.000251     0.025409     0.111470     0.405763     0.929889   \n",
       "25%       0.890477     0.920497     0.926656     1.331999     1.945007   \n",
       "50%       2.199088     2.226364     2.215159     2.399267     2.764713   \n",
       "75%       4.311375     4.287757     4.324418     3.921087     3.628557   \n",
       "max       7.057885     6.828700     6.745545     6.182137     4.180595   \n",
       "\n",
       "            rmse_1       rmse_3       rmse_5      rmse_21      rmse_63  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean     17.597263    30.287632    38.962604    79.783066   139.389124  \n",
       "std      13.300598    22.604696    28.688250    50.138198    51.022857  \n",
       "min       0.001597     0.000309     0.053601     0.032072    37.661023  \n",
       "25%       5.711175    10.076860    12.753381    37.575051   100.083634  \n",
       "50%      14.252118    24.856386    32.151016    72.517032   142.228716  \n",
       "75%      28.566812    49.420654    64.186421   119.145236   189.809872  \n",
       "max      47.637195    79.638420   101.418766   189.323809   220.316387  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mape_1</th>\n      <th>mape_3</th>\n      <th>mape_5</th>\n      <th>mape_21</th>\n      <th>mape_63</th>\n      <th>rmse_1</th>\n      <th>rmse_3</th>\n      <th>rmse_5</th>\n      <th>rmse_21</th>\n      <th>rmse_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>2.658915</td>\n      <td>2.664283</td>\n      <td>2.669997</td>\n      <td>2.694996</td>\n      <td>2.713831</td>\n      <td>17.597263</td>\n      <td>30.287632</td>\n      <td>38.962604</td>\n      <td>79.783066</td>\n      <td>139.389124</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>1.975212</td>\n      <td>1.917600</td>\n      <td>1.871873</td>\n      <td>1.570899</td>\n      <td>0.920836</td>\n      <td>13.300598</td>\n      <td>22.604696</td>\n      <td>28.688250</td>\n      <td>50.138198</td>\n      <td>51.022857</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000251</td>\n      <td>0.025409</td>\n      <td>0.111470</td>\n      <td>0.405763</td>\n      <td>0.929889</td>\n      <td>0.001597</td>\n      <td>0.000309</td>\n      <td>0.053601</td>\n      <td>0.032072</td>\n      <td>37.661023</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.890477</td>\n      <td>0.920497</td>\n      <td>0.926656</td>\n      <td>1.331999</td>\n      <td>1.945007</td>\n      <td>5.711175</td>\n      <td>10.076860</td>\n      <td>12.753381</td>\n      <td>37.575051</td>\n      <td>100.083634</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>2.199088</td>\n      <td>2.226364</td>\n      <td>2.215159</td>\n      <td>2.399267</td>\n      <td>2.764713</td>\n      <td>14.252118</td>\n      <td>24.856386</td>\n      <td>32.151016</td>\n      <td>72.517032</td>\n      <td>142.228716</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>4.311375</td>\n      <td>4.287757</td>\n      <td>4.324418</td>\n      <td>3.921087</td>\n      <td>3.628557</td>\n      <td>28.566812</td>\n      <td>49.420654</td>\n      <td>64.186421</td>\n      <td>119.145236</td>\n      <td>189.809872</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.057885</td>\n      <td>6.828700</td>\n      <td>6.745545</td>\n      <td>6.182137</td>\n      <td>4.180595</td>\n      <td>47.637195</td>\n      <td>79.638420</td>\n      <td>101.418766</td>\n      <td>189.323809</td>\n      <td>220.316387</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "cross_df.iloc[:, :10].describe()"
   ]
  },
  {
   "source": [
    "## Description of precision, recall and fscore"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       precision_1  precision_3  precision_5  precision_21  precision_63  \\\n",
       "count  1000.000000  1000.000000  1000.000000   1000.000000   1000.000000   \n",
       "mean      0.405000     0.464167     0.490250      0.559407      0.560880   \n",
       "std       0.491138     0.370424     0.321948      0.199862      0.048775   \n",
       "min       0.000000     0.000000     0.000000      0.000000      0.476190   \n",
       "25%       0.000000     0.000000     0.200000      0.444444      0.520000   \n",
       "50%       0.000000     0.333333     0.600000      0.575188      0.563636   \n",
       "75%       1.000000     0.666667     0.750000      0.684211      0.600000   \n",
       "max       1.000000     1.000000     1.000000      1.000000      0.659574   \n",
       "\n",
       "          recall_1     recall_3     recall_5    recall_21    recall_63  \\\n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
       "mean      0.405000     0.668833     0.739083     0.736007     0.729661   \n",
       "std       0.491138     0.447960     0.396280     0.294759     0.155639   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.441176   \n",
       "25%       0.000000     0.000000     0.500000     0.538462     0.617647   \n",
       "50%       0.000000     1.000000     1.000000     0.857143     0.714286   \n",
       "75%       1.000000     1.000000     1.000000     1.000000     0.861111   \n",
       "max       1.000000     1.000000     1.000000     1.000000     0.973684   \n",
       "\n",
       "          fscore_1     fscore_3     fscore_5    fscore_21    fscore_63  \n",
       "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
       "mean      0.405000     0.513833     0.544937     0.592908     0.626730  \n",
       "std       0.491138     0.364848     0.311632     0.190749     0.073600  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.468750  \n",
       "25%       0.000000     0.000000     0.333333     0.518519     0.571429  \n",
       "50%       0.000000     0.500000     0.666667     0.640000     0.607595  \n",
       "75%       1.000000     0.800000     0.750000     0.733333     0.695652  \n",
       "max       1.000000     1.000000     1.000000     0.857143     0.756098  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>precision_1</th>\n      <th>precision_3</th>\n      <th>precision_5</th>\n      <th>precision_21</th>\n      <th>precision_63</th>\n      <th>recall_1</th>\n      <th>recall_3</th>\n      <th>recall_5</th>\n      <th>recall_21</th>\n      <th>recall_63</th>\n      <th>fscore_1</th>\n      <th>fscore_3</th>\n      <th>fscore_5</th>\n      <th>fscore_21</th>\n      <th>fscore_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n      <td>1000.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.405000</td>\n      <td>0.464167</td>\n      <td>0.490250</td>\n      <td>0.559407</td>\n      <td>0.560880</td>\n      <td>0.405000</td>\n      <td>0.668833</td>\n      <td>0.739083</td>\n      <td>0.736007</td>\n      <td>0.729661</td>\n      <td>0.405000</td>\n      <td>0.513833</td>\n      <td>0.544937</td>\n      <td>0.592908</td>\n      <td>0.626730</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.491138</td>\n      <td>0.370424</td>\n      <td>0.321948</td>\n      <td>0.199862</td>\n      <td>0.048775</td>\n      <td>0.491138</td>\n      <td>0.447960</td>\n      <td>0.396280</td>\n      <td>0.294759</td>\n      <td>0.155639</td>\n      <td>0.491138</td>\n      <td>0.364848</td>\n      <td>0.311632</td>\n      <td>0.190749</td>\n      <td>0.073600</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.476190</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.441176</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.468750</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.200000</td>\n      <td>0.444444</td>\n      <td>0.520000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.538462</td>\n      <td>0.617647</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.333333</td>\n      <td>0.518519</td>\n      <td>0.571429</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.000000</td>\n      <td>0.333333</td>\n      <td>0.600000</td>\n      <td>0.575188</td>\n      <td>0.563636</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.857143</td>\n      <td>0.714286</td>\n      <td>0.000000</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>0.640000</td>\n      <td>0.607595</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>1.000000</td>\n      <td>0.666667</td>\n      <td>0.750000</td>\n      <td>0.684211</td>\n      <td>0.600000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.861111</td>\n      <td>1.000000</td>\n      <td>0.800000</td>\n      <td>0.750000</td>\n      <td>0.733333</td>\n      <td>0.695652</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.659574</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.973684</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.857143</td>\n      <td>0.756098</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "cross_df.iloc[:, 10:].describe()"
   ]
  },
  {
   "source": [
    "## Confidence intervals for RMSE, MAPE, precision, recall and fscore"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         measure        mean       lower       upper\n",
       "0         mape_1    2.658915    2.536490    2.781340\n",
       "1         mape_3    2.664283    2.545429    2.783137\n",
       "2         mape_5    2.669997    2.553977    2.786017\n",
       "3        mape_21    2.694996    2.597631    2.792362\n",
       "4        mape_63    2.713831    2.656757    2.770905\n",
       "5         rmse_1   17.597263   16.772883   18.421643\n",
       "6         rmse_3   30.287632   28.886579   31.688686\n",
       "7         rmse_5   38.962604   37.184488   40.740721\n",
       "8        rmse_21   79.783066   76.675468   82.890664\n",
       "9        rmse_63  139.389124  136.226694  142.551553\n",
       "10   precision_1    0.405000    0.374559    0.435441\n",
       "11   precision_3    0.464167    0.441208    0.487126\n",
       "12   precision_5    0.490250    0.470295    0.510205\n",
       "13  precision_21    0.559407    0.547019    0.571794\n",
       "14  precision_63    0.560880    0.557857    0.563903\n",
       "15      recall_1    0.405000    0.374559    0.435441\n",
       "16      recall_3    0.668833    0.641068    0.696598\n",
       "17      recall_5    0.739083    0.714522    0.763645\n",
       "18     recall_21    0.736007    0.717738    0.754277\n",
       "19     recall_63    0.729661    0.720015    0.739308\n",
       "20      fscore_1    0.405000    0.374559    0.435441\n",
       "21      fscore_3    0.513833    0.491220    0.536447\n",
       "22      fscore_5    0.544937    0.525622    0.564252\n",
       "23     fscore_21    0.592908    0.581085    0.604731\n",
       "24     fscore_63    0.626730    0.622168    0.631292"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>measure</th>\n      <th>mean</th>\n      <th>lower</th>\n      <th>upper</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mape_1</td>\n      <td>2.658915</td>\n      <td>2.536490</td>\n      <td>2.781340</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mape_3</td>\n      <td>2.664283</td>\n      <td>2.545429</td>\n      <td>2.783137</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>mape_5</td>\n      <td>2.669997</td>\n      <td>2.553977</td>\n      <td>2.786017</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>mape_21</td>\n      <td>2.694996</td>\n      <td>2.597631</td>\n      <td>2.792362</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>mape_63</td>\n      <td>2.713831</td>\n      <td>2.656757</td>\n      <td>2.770905</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>rmse_1</td>\n      <td>17.597263</td>\n      <td>16.772883</td>\n      <td>18.421643</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>rmse_3</td>\n      <td>30.287632</td>\n      <td>28.886579</td>\n      <td>31.688686</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>rmse_5</td>\n      <td>38.962604</td>\n      <td>37.184488</td>\n      <td>40.740721</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>rmse_21</td>\n      <td>79.783066</td>\n      <td>76.675468</td>\n      <td>82.890664</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>rmse_63</td>\n      <td>139.389124</td>\n      <td>136.226694</td>\n      <td>142.551553</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>precision_1</td>\n      <td>0.405000</td>\n      <td>0.374559</td>\n      <td>0.435441</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>precision_3</td>\n      <td>0.464167</td>\n      <td>0.441208</td>\n      <td>0.487126</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>precision_5</td>\n      <td>0.490250</td>\n      <td>0.470295</td>\n      <td>0.510205</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>precision_21</td>\n      <td>0.559407</td>\n      <td>0.547019</td>\n      <td>0.571794</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>precision_63</td>\n      <td>0.560880</td>\n      <td>0.557857</td>\n      <td>0.563903</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>recall_1</td>\n      <td>0.405000</td>\n      <td>0.374559</td>\n      <td>0.435441</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>recall_3</td>\n      <td>0.668833</td>\n      <td>0.641068</td>\n      <td>0.696598</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>recall_5</td>\n      <td>0.739083</td>\n      <td>0.714522</td>\n      <td>0.763645</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>recall_21</td>\n      <td>0.736007</td>\n      <td>0.717738</td>\n      <td>0.754277</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>recall_63</td>\n      <td>0.729661</td>\n      <td>0.720015</td>\n      <td>0.739308</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>fscore_1</td>\n      <td>0.405000</td>\n      <td>0.374559</td>\n      <td>0.435441</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>fscore_3</td>\n      <td>0.513833</td>\n      <td>0.491220</td>\n      <td>0.536447</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>fscore_5</td>\n      <td>0.544937</td>\n      <td>0.525622</td>\n      <td>0.564252</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>fscore_21</td>\n      <td>0.592908</td>\n      <td>0.581085</td>\n      <td>0.604731</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>fscore_63</td>\n      <td>0.626730</td>\n      <td>0.622168</td>\n      <td>0.631292</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "n = cross_df.count()[0]\n",
    "mean = cross_df.mean()\n",
    "upper = cross_df.mean() + 1.96 * cross_df.std() / math.sqrt(n)\n",
    "lower = cross_df.mean() - 1.96 * cross_df.std() / math.sqrt(n)\n",
    "\n",
    "ci_df = pd.DataFrame(columns=['measure', 'mean', 'lower', 'upper'])\n",
    "\n",
    "for i in range(25):\n",
    "    ci_df = ci_df.append({\n",
    "        'measure': cross_df.columns[i],\n",
    "        'mean': mean[i],\n",
    "        'lower': lower[i],\n",
    "        'upper': upper[i]\n",
    "    }, ignore_index=True)\n",
    "\n",
    "ci_df"
   ]
  }
 ]
}