{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd03c80b69efc6c35edf58052dd52e794939bcc5e24ed141d0f365672b2a75355b3",
   "display_name": "Python 3.8.8 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "source": [
    "# Creating functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(\"var%d(t-%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(\"var%d(t)\" % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(\"var%d(t+%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "\n",
    "    # transform data to be stationary\n",
    "    diff_values = raw_values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, verbose=2, shuffle=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq, forecast_len):\n",
    "    forecasts = list()\n",
    "    print(f'Forecast x of {forecast_len}:', end=\" \")\n",
    "    for i in range(forecast_len):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "\n",
    "        # Printing current status in hundreds\n",
    "        step = i % 10\n",
    "        if step == 0:\n",
    "            print(i, end=\" \")\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "\n",
    "        inverted.append(inv_scale)\n",
    "\n",
    "    return inverted"
   ]
  },
  {
   "source": [
    "# Fitting and predicting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing data...\n",
      "Fitting model...\n",
      "Epoch 1/5\n",
      "3360/3360 - 5s - loss: 0.0090\n",
      "Epoch 2/5\n",
      "3360/3360 - 3s - loss: 0.0078\n",
      "Epoch 3/5\n",
      "3360/3360 - 3s - loss: 0.0077\n",
      "Epoch 4/5\n",
      "3360/3360 - 3s - loss: 0.0077\n",
      "Epoch 5/5\n",
      "3360/3360 - 3s - loss: 0.0077\n",
      "Making forecasts...\n",
      "Forecast x of 100: 0 10 20 30 40 50 60 70 80 90 \n",
      "Inverting forecasts...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "logreturns = \"data/final.csv\"\n",
    "series = pd.read_csv(logreturns, usecols=[\"Exchange.Date\", \"logreturns\"], header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# configure\n",
    "n_lag = 1 # same as ARMA-GARCH\n",
    "n_seq = 63  #  number of periods forecast\n",
    "test_share = 0.25\n",
    "n_test = int(len(series) * test_share)\n",
    "n_epochs = 5\n",
    "n_batch = 1\n",
    "n_neurons = 50\n",
    "forecast_len = 100\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "print(\"Making forecasts...\")\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq, forecast_len)\n",
    "\n",
    "print(\"\\nInverting forecasts...\")\n",
    "forecasts = inverse_transform(series, forecasts, scaler)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "# Evaluating from t=1\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Creating dataframe for evaluation\n",
    "In essence, creating a new DF combining training data (historic) and forecasts"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting dataframe with Close as well and the creating a training df same size as used in the model\n",
    "original_df = pd.read_csv(\"data/final.csv\", usecols=[\"Exchange.Date\", \"logreturns\", \"Close\"])\n",
    "\n",
    "# Setting as date\n",
    "original_df['Exchange.Date'] = original_df['Exchange.Date'].apply(lambda x: date(1900, 1, 1) + timedelta(int(x)))\n",
    "original_df.index = original_df['Exchange.Date']\n",
    "\n",
    "train_df = original_df[:-n_test].copy()\n",
    "\n",
    "# Assigning all rows in train df (before forecast) to closing value\n",
    "# This is because this column cannot be empty (and we have no forecasts since it's training data)\n",
    "train_df[\"forecast\"] = train_df[\"Close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming logreturns back to price\n",
    "last_train = train_df[\"Close\"].values[-1]\n",
    "price_forecasts = np.exp(np.cumsum(forecasts[0]) + math.log(last_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a separate dataframe only for forecasts (i.e. \"outside train df\")\n",
    "forecast_df = pd.DataFrame(columns=[\"Exchange.Date\", \"Close\", \"logreturns\", \"forecast\"])\n",
    "forecast_df[\"Close\"] = original_df[\"Close\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"logreturns\"] = original_df[\"logreturns\"].values[-n_test : -n_test + n_seq]\n",
    "forecast_df[\"forecast\"] = price_forecasts\n",
    "forecast_df.index\n",
    "\n",
    "forecast_df[\"Exchange.Date\"] = forecast_df.index.map(lambda x: date(2016, 8, 1) + timedelta(int(x)))\n",
    "forecast_df.index = forecast_df[\"Exchange.Date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "              Exchange.Date   Close  logreturns    forecast      error  \\\n",
       "Exchange.Date                                                            \n",
       "2016-09-28       2016-09-28  671.89    0.005567  628.119507 -43.770493   \n",
       "2016-09-29       2016-09-29  671.08   -0.001206  629.037231 -42.042769   \n",
       "2016-09-30       2016-09-30  668.82   -0.003373  629.909790 -38.910210   \n",
       "2016-10-01       2016-10-01  668.02   -0.001197  630.731567 -37.288433   \n",
       "2016-10-02       2016-10-02  663.33   -0.007046  631.608887 -31.721113   \n",
       "\n",
       "               abs_error  actual_up  forecast_up confusion  \n",
       "Exchange.Date                                               \n",
       "2016-09-28     43.770493       True         True        TP  \n",
       "2016-09-29     42.042769      False         True        FP  \n",
       "2016-09-30     38.910210      False         True        FP  \n",
       "2016-10-01     37.288433      False         True        FP  \n",
       "2016-10-02     31.721113      False         True        FP  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Exchange.Date</th>\n      <th>Close</th>\n      <th>logreturns</th>\n      <th>forecast</th>\n      <th>error</th>\n      <th>abs_error</th>\n      <th>actual_up</th>\n      <th>forecast_up</th>\n      <th>confusion</th>\n    </tr>\n    <tr>\n      <th>Exchange.Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2016-09-28</th>\n      <td>2016-09-28</td>\n      <td>671.89</td>\n      <td>0.005567</td>\n      <td>628.119507</td>\n      <td>-43.770493</td>\n      <td>43.770493</td>\n      <td>True</td>\n      <td>True</td>\n      <td>TP</td>\n    </tr>\n    <tr>\n      <th>2016-09-29</th>\n      <td>2016-09-29</td>\n      <td>671.08</td>\n      <td>-0.001206</td>\n      <td>629.037231</td>\n      <td>-42.042769</td>\n      <td>42.042769</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-09-30</th>\n      <td>2016-09-30</td>\n      <td>668.82</td>\n      <td>-0.003373</td>\n      <td>629.909790</td>\n      <td>-38.910210</td>\n      <td>38.910210</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-10-01</th>\n      <td>2016-10-01</td>\n      <td>668.02</td>\n      <td>-0.001197</td>\n      <td>630.731567</td>\n      <td>-37.288433</td>\n      <td>37.288433</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n    <tr>\n      <th>2016-10-02</th>\n      <td>2016-10-02</td>\n      <td>663.33</td>\n      <td>-0.007046</td>\n      <td>631.608887</td>\n      <td>-31.721113</td>\n      <td>31.721113</td>\n      <td>False</td>\n      <td>True</td>\n      <td>FP</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# Merging train and forecast dataframe\n",
    "merged_df = train_df.append(forecast_df, ignore_index=True)\n",
    "\n",
    "# Creating error, absolute error, actual price going up (True/False) and forecast going up (True/False)\n",
    "merged_df[\"error\"] = merged_df[\"forecast\"] - merged_df[\"Close\"]\n",
    "merged_df[\"abs_error\"] = np.abs(merged_df[\"forecast\"] - merged_df[\"Close\"])\n",
    "merged_df[\"actual_up\"] = merged_df[\"Close\"].diff(1) > 0\n",
    "merged_df[\"forecast_up\"] = merged_df[\"forecast\"].diff(1) > 0\n",
    "merged_df.index = merged_df[\"Exchange.Date\"]\n",
    "\n",
    "# Formula for creating confusion value, used below\n",
    "def confusion(actual, forecast):\n",
    "    if actual and forecast:\n",
    "        return \"TP\"\n",
    "\n",
    "    if actual and not forecast:\n",
    "        return \"FN\"\n",
    "\n",
    "    if not actual and forecast:\n",
    "        return \"FP\"\n",
    "\n",
    "    if not actual and not forecast:\n",
    "        return \"TN\"\n",
    "\n",
    "    # Just common programming sense to return something, could have written \"blabla\"\n",
    "    return False\n",
    "\n",
    "\n",
    "# The lambda stuff applies the above function on every row of data\n",
    "merged_df[\"confusion\"] = merged_df.apply(lambda x: confusion(x[\"actual_up\"], x[\"forecast_up\"]), axis=1)\n",
    "\n",
    "# Printing the tail of the data\n",
    "merged_df.tail()"
   ]
  },
  {
   "source": [
    "## Evaluating"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframe that only contains the number of periods to evaluate (1,3,5,21,63)\n",
    "def new_df(n_periods):\n",
    "    df = merged_df[len(train_df) : len(train_df) + n_periods]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1, RMSE: 1.43, MAPE: 0.23%\n3, RMSE: 3.497, MAPE: 0.481%\n5, RMSE: 3.422, MAPE: 0.34%\n21, RMSE: 26.242, MAPE: 1.039%\n63, RMSE: 175.119, MAPE: 3.394%\n"
     ]
    }
   ],
   "source": [
    "# Creating RMSE AND MAE\n",
    "def evaluate(n_periods):\n",
    "    df = new_df(n_periods)\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    print(f\"{n_periods}, RMSE: {round(rmse, 3)}, MAPE: {round(mape, 3)}%\")\n",
    "\n",
    "\n",
    "evaluate(1)  # 1 day\n",
    "evaluate(3)  # half a week\n",
    "evaluate(5)  # week\n",
    "evaluate(21)  # month\n",
    "evaluate(63)  # quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating confusion matrix\n",
    "def confusion_matrix(df):\n",
    "    conf = pd.DataFrame(columns=[\"P\", \"N\"], index=[\"P\", \"N\"])\n",
    "    conf.loc[\"P\", \"P\"] = len(df[df[\"confusion\"] == \"TP\"])\n",
    "    conf.loc[\"P\", \"N\"] = len(df[df[\"confusion\"] == \"FN\"])\n",
    "    conf.loc[\"N\", \"P\"] = len(df[df[\"confusion\"] == \"FP\"])\n",
    "    conf.loc[\"N\", \"N\"] = len(df[df[\"confusion\"] == \"TN\"])\n",
    "    return conf\n",
    "\n",
    "\n",
    "confusion = confusion_matrix(new_df(63))\n",
    "precision = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[0, 1])\n",
    "recall = confusion.iloc[0, 0] / (confusion.iloc[0, 0] + confusion.iloc[1, 0])\n",
    "f_score = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(confusion)\n",
    "print(f\"precision: {int(precision*100)}%, recall: {int(recall*100)}%, f-score: {round(f_score, 3)}\")"
   ]
  },
  {
   "source": [
    "# Plotting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = merged_df[-n_seq - (n_seq * 2) :]\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(plot_df[\"forecast\"], label=\"forecast\")\n",
    "plt.plot(plot_df[\"Close\"], label=\"actual\")\n",
    "plt.legend()"
   ]
  },
  {
   "source": [
    "# Cross-validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      mape_1    mape_3    mape_5   mape_21   mape_63    rmse_1     rmse_3  \\\n",
       "0   0.229605  0.480505  0.339661  1.038823  3.371268  1.429912   3.496537   \n",
       "1   0.687607  0.670457  0.449289  1.233065  3.956861  4.254224   7.180921   \n",
       "2   0.262093  0.365631  0.485001  1.464351  3.573292  1.617427   2.063352   \n",
       "3   0.637265  0.795726  0.882841  2.066800  4.670550  3.959199   8.583362   \n",
       "4   0.124481  0.334379  0.539262  1.927808  5.077635  0.774661   3.615796   \n",
       "..       ...       ...       ...       ...       ...       ...        ...   \n",
       "95  0.051701  0.469304  0.996892  2.801478  8.272004  0.359170   5.694220   \n",
       "96  0.253303  0.906831  1.271988  3.050168  8.475108  1.764407  11.044099   \n",
       "97  0.758554  1.243447  1.186724  3.075397  8.357846  5.324368  15.212916   \n",
       "98  0.449671  0.678716  0.588132  2.634286  7.943590  3.170720   8.315173   \n",
       "99  0.551257  0.342678  0.324811  2.615952  8.055193  3.908960   1.227375   \n",
       "\n",
       "       rmse_5     rmse_21     rmse_63  \n",
       "0    3.422311   26.241554  172.497399  \n",
       "1    6.192209   29.837768  201.701011  \n",
       "2    5.307778   41.930135  184.809754  \n",
       "3   12.313697   60.162459  241.961192  \n",
       "4    7.559783   56.232704  263.478414  \n",
       "..        ...         ...         ...  \n",
       "95  15.725754   92.503383  501.588209  \n",
       "96  20.073864  100.889347  514.280150  \n",
       "97  18.726793  102.103692  507.985187  \n",
       "98   9.300342   87.957770  484.185278  \n",
       "99   1.431479   85.838929  490.786992  \n",
       "\n",
       "[100 rows x 10 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mape_1</th>\n      <th>mape_3</th>\n      <th>mape_5</th>\n      <th>mape_21</th>\n      <th>mape_63</th>\n      <th>rmse_1</th>\n      <th>rmse_3</th>\n      <th>rmse_5</th>\n      <th>rmse_21</th>\n      <th>rmse_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.229605</td>\n      <td>0.480505</td>\n      <td>0.339661</td>\n      <td>1.038823</td>\n      <td>3.371268</td>\n      <td>1.429912</td>\n      <td>3.496537</td>\n      <td>3.422311</td>\n      <td>26.241554</td>\n      <td>172.497399</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.687607</td>\n      <td>0.670457</td>\n      <td>0.449289</td>\n      <td>1.233065</td>\n      <td>3.956861</td>\n      <td>4.254224</td>\n      <td>7.180921</td>\n      <td>6.192209</td>\n      <td>29.837768</td>\n      <td>201.701011</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.262093</td>\n      <td>0.365631</td>\n      <td>0.485001</td>\n      <td>1.464351</td>\n      <td>3.573292</td>\n      <td>1.617427</td>\n      <td>2.063352</td>\n      <td>5.307778</td>\n      <td>41.930135</td>\n      <td>184.809754</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.637265</td>\n      <td>0.795726</td>\n      <td>0.882841</td>\n      <td>2.066800</td>\n      <td>4.670550</td>\n      <td>3.959199</td>\n      <td>8.583362</td>\n      <td>12.313697</td>\n      <td>60.162459</td>\n      <td>241.961192</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.124481</td>\n      <td>0.334379</td>\n      <td>0.539262</td>\n      <td>1.927808</td>\n      <td>5.077635</td>\n      <td>0.774661</td>\n      <td>3.615796</td>\n      <td>7.559783</td>\n      <td>56.232704</td>\n      <td>263.478414</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>0.051701</td>\n      <td>0.469304</td>\n      <td>0.996892</td>\n      <td>2.801478</td>\n      <td>8.272004</td>\n      <td>0.359170</td>\n      <td>5.694220</td>\n      <td>15.725754</td>\n      <td>92.503383</td>\n      <td>501.588209</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>0.253303</td>\n      <td>0.906831</td>\n      <td>1.271988</td>\n      <td>3.050168</td>\n      <td>8.475108</td>\n      <td>1.764407</td>\n      <td>11.044099</td>\n      <td>20.073864</td>\n      <td>100.889347</td>\n      <td>514.280150</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>0.758554</td>\n      <td>1.243447</td>\n      <td>1.186724</td>\n      <td>3.075397</td>\n      <td>8.357846</td>\n      <td>5.324368</td>\n      <td>15.212916</td>\n      <td>18.726793</td>\n      <td>102.103692</td>\n      <td>507.985187</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>0.449671</td>\n      <td>0.678716</td>\n      <td>0.588132</td>\n      <td>2.634286</td>\n      <td>7.943590</td>\n      <td>3.170720</td>\n      <td>8.315173</td>\n      <td>9.300342</td>\n      <td>87.957770</td>\n      <td>484.185278</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>0.551257</td>\n      <td>0.342678</td>\n      <td>0.324811</td>\n      <td>2.615952</td>\n      <td>8.055193</td>\n      <td>3.908960</td>\n      <td>1.227375</td>\n      <td>1.431479</td>\n      <td>85.838929</td>\n      <td>490.786992</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "def evaluate(df, n_periods):\n",
    "    n_periods = 62 if n_periods == 63 else n_periods\n",
    "    df = df[-63:-63+n_periods]\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    return mape, rmse\n",
    "\n",
    "cross_df = pd.DataFrame(columns=[\n",
    "    \"mape_1\", \n",
    "    \"mape_3\",\n",
    "    \"mape_5\",\n",
    "    \"mape_21\",\n",
    "    \"mape_63\",\n",
    "    \"rmse_1\",\n",
    "    \"rmse_3\",\n",
    "    \"rmse_5\",\n",
    "    \"rmse_21\",\n",
    "    \"rmse_63\"\n",
    "])\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    train_df = original_df[:-n_test + i].copy()\n",
    "    train_df[\"forecast\"] = train_df[\"Close\"]\n",
    "\n",
    "    last_train = train_df[\"Close\"].values[-1]\n",
    "    price_forecasts = np.exp(np.cumsum(forecasts[i]) + math.log(last_train))\n",
    "\n",
    "    cross_merged_df = original_df[-n_test + i:-n_test + i + 63].copy()\n",
    "    cross_merged_df[\"forecast\"] = price_forecasts\n",
    "\n",
    "    cross_merged_df[\"error\"] = cross_merged_df[\"forecast\"] - cross_merged_df[\"Close\"]\n",
    "    cross_merged_df[\"abs_error\"] = np.abs(cross_merged_df[\"forecast\"] - cross_merged_df[\"Close\"])\n",
    "\n",
    "    one = evaluate(cross_merged_df, 1)\n",
    "    three = evaluate(cross_merged_df, 3)\n",
    "    five = evaluate(cross_merged_df, 5)\n",
    "    twentyone = evaluate(cross_merged_df, 21)\n",
    "    sixtythree = evaluate(cross_merged_df, 63)\n",
    "\n",
    "    cross_df = cross_df.append({\n",
    "        'mape_1': one[0],\n",
    "        'mape_3': three[0],\n",
    "        'mape_5': five[0],\n",
    "        'mape_21': twentyone[0],\n",
    "        'mape_63': sixtythree[0],\n",
    "        'rmse_1': one[1],\n",
    "        'rmse_3': three[1],\n",
    "        'rmse_5': five[1],\n",
    "        'rmse_21': twentyone[1],\n",
    "        'rmse_63': sixtythree[1],\n",
    "    }, ignore_index=True)\n",
    "\n",
    "cross_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}