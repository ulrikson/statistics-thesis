{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python388jvsc74a57bd03c80b69efc6c35edf58052dd52e794939bcc5e24ed141d0f365672b2a75355b3",
   "display_name": "Python 3.8.8 64-bit ('venv')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## NOTE: This should not be run as a separate file, will include in lstm2.ipynb later\n",
    "This is just for testing and not screwing anything working up.\n",
    "\n",
    "Remember:\n",
    "\n",
    "- Changed len_forecasts to 100 instead of len(test)\n",
    "- Ignored date transformation as we don't have to make graphs\n",
    "- And only RMSE/MAPE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/multi-step-time-series-forecasting-long-short-term-memory-networks-python/\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "source": [
    "# Creating functions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [(\"var%d(t-%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [(\"var%d(t)\" % (j + 1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [(\"var%d(t+%d)\" % (j + 1, i)) for j in range(n_vars)]\n",
    "\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "\n",
    "    return agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform series into train and test sets for supervised learning\n",
    "def prepare_data(series, n_test, n_lag, n_seq):\n",
    "    # extract raw values\n",
    "    raw_values = series.values\n",
    "\n",
    "    # transform data to be stationary\n",
    "    diff_values = raw_values\n",
    "    diff_values = diff_values.reshape(len(diff_values), 1)\n",
    "\n",
    "    # rescale values to -1, 1\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_values = scaler.fit_transform(diff_values)\n",
    "    scaled_values = scaled_values.reshape(len(scaled_values), 1)\n",
    "\n",
    "    # transform into supervised learning problem X, y\n",
    "    supervised = series_to_supervised(scaled_values, n_lag, n_seq)\n",
    "    supervised_values = supervised.values\n",
    "\n",
    "    # split into train and test sets\n",
    "    train, test = supervised_values[0:-n_test], supervised_values[-n_test:]\n",
    "    return scaler, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_lstm(train, n_lag, n_seq, n_batch, nb_epoch, n_neurons):\n",
    "    # reshape training into [samples, timesteps, features]\n",
    "    X, y = train[:, 0:n_lag], train[:, n_lag:]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "\n",
    "    # design network\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(n_neurons, batch_input_shape=(n_batch, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(y.shape[1]))\n",
    "    model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=n_batch, verbose=2, shuffle=False)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make one forecast with an LSTM,\n",
    "def forecast_lstm(model, X, n_batch):\n",
    "    # reshape input pattern to [samples, timesteps, features]\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "\n",
    "    # make forecast\n",
    "    forecast = model.predict(X, batch_size=n_batch)\n",
    "\n",
    "    # convert to array\n",
    "    return [x for x in forecast[0, :]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the persistence model\n",
    "def make_forecasts(model, n_batch, train, test, n_lag, n_seq):\n",
    "    forecasts = list()\n",
    "    forecast_len = 100\n",
    "\n",
    "    print(f'Forecast x of {forecast_len}:', end=\" \")\n",
    "    for i in range(forecast_len):\n",
    "        X, y = test[i, 0:n_lag], test[i, n_lag:]\n",
    "        # make forecast\n",
    "        forecast = forecast_lstm(model, X, n_batch)\n",
    "        # store the forecast\n",
    "        forecasts.append(forecast)\n",
    "\n",
    "        # Printing current status in tens\n",
    "        tens = i % 10\n",
    "        if tens == 0:\n",
    "            print(i, end=\" \")\n",
    "\n",
    "    return forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse data transform on forecasts\n",
    "def inverse_transform(series, forecasts, scaler):\n",
    "    inverted = list()\n",
    "    for i in range(len(forecasts)):\n",
    "\n",
    "        # create array from forecast\n",
    "        forecast = np.array(forecasts[i])\n",
    "        forecast = forecast.reshape(1, len(forecast))\n",
    "\n",
    "        # invert scaling\n",
    "        inv_scale = scaler.inverse_transform(forecast)\n",
    "        inv_scale = inv_scale[0, :]\n",
    "\n",
    "        inverted.append(inv_scale)\n",
    "\n",
    "    return inverted"
   ]
  },
  {
   "source": [
    "# Fitting and predicting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Preparing data...\n",
      "Fitting model...\n",
      "Epoch 1/5\n",
      "3360/3360 - 5s - loss: 0.0090\n",
      "Epoch 2/5\n",
      "3360/3360 - 3s - loss: 0.0078\n",
      "Epoch 3/5\n",
      "3360/3360 - 3s - loss: 0.0077\n",
      "Epoch 4/5\n",
      "3360/3360 - 3s - loss: 0.0077\n",
      "Epoch 5/5\n",
      "3360/3360 - 3s - loss: 0.0077\n",
      "Making forecasts...\n",
      "Forecast x of 100: 0 10 20 30 40 50 60 70 80 90 \n",
      "Inverting forecasts...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "logreturns = \"data/final.csv\"\n",
    "series = pd.read_csv(logreturns, usecols=[\"Exchange.Date\", \"logreturns\"], header=0, index_col=0, squeeze=True)\n",
    "\n",
    "# configure\n",
    "n_lag = 1 # same as ARMA-GARCH\n",
    "n_seq = 63  #  number of periods forecast\n",
    "test_share = 0.25\n",
    "n_test = int(len(series) * test_share)\n",
    "n_epochs = 5\n",
    "n_batch = 1\n",
    "n_neurons = 50\n",
    "\n",
    "print(\"Preparing data...\")\n",
    "scaler, train, test = prepare_data(series, n_test, n_lag, n_seq)\n",
    "\n",
    "print(\"Fitting model...\")\n",
    "model = fit_lstm(train, n_lag, n_seq, n_batch, n_epochs, n_neurons)\n",
    "\n",
    "print(\"Making forecasts...\")\n",
    "forecasts = make_forecasts(model, n_batch, train, test, n_lag, n_seq)\n",
    "\n",
    "print(\"\\nInverting forecasts...\")\n",
    "forecasts = inverse_transform(series, forecasts, scaler)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "source": [
    "# Cross Evaluating 100 periods"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                     Close  logreturns\n",
       "Exchange.Date                         \n",
       "37620           100.000000    0.000000\n",
       "37623           100.995362    0.009904\n",
       "37624           101.623083    0.006196\n",
       "37627           101.623083    0.000000\n",
       "37628           102.392342    0.007541\n",
       "...                    ...         ...\n",
       "44235          1249.100000    0.005668\n",
       "44236          1245.630000   -0.002782\n",
       "44237          1243.380000   -0.001808\n",
       "44238          1254.090000    0.008577\n",
       "44239          1253.630000   -0.000367\n",
       "\n",
       "[4564 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Close</th>\n      <th>logreturns</th>\n    </tr>\n    <tr>\n      <th>Exchange.Date</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37620</th>\n      <td>100.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>37623</th>\n      <td>100.995362</td>\n      <td>0.009904</td>\n    </tr>\n    <tr>\n      <th>37624</th>\n      <td>101.623083</td>\n      <td>0.006196</td>\n    </tr>\n    <tr>\n      <th>37627</th>\n      <td>101.623083</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>37628</th>\n      <td>102.392342</td>\n      <td>0.007541</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>44235</th>\n      <td>1249.100000</td>\n      <td>0.005668</td>\n    </tr>\n    <tr>\n      <th>44236</th>\n      <td>1245.630000</td>\n      <td>-0.002782</td>\n    </tr>\n    <tr>\n      <th>44237</th>\n      <td>1243.380000</td>\n      <td>-0.001808</td>\n    </tr>\n    <tr>\n      <th>44238</th>\n      <td>1254.090000</td>\n      <td>0.008577</td>\n    </tr>\n    <tr>\n      <th>44239</th>\n      <td>1253.630000</td>\n      <td>-0.000367</td>\n    </tr>\n  </tbody>\n</table>\n<p>4564 rows Ã— 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "original_df = pd.read_csv(\"data/final.csv\", usecols=[\"Exchange.Date\", \"logreturns\", \"Close\"])\n",
    "original_df.index = original_df['Exchange.Date']\n",
    "original_df.drop('Exchange.Date', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     mape_1    mape_3    mape_5   mape_21   mape_63    rmse_1    rmse_3  \\\n",
       "0  0.174163  0.548679  0.452738  0.878127  3.019335  1.084636  4.623117   \n",
       "1  0.696150  0.701604  0.485295  1.201450  3.898030  4.307080  7.515089   \n",
       "2  0.319051  0.295229  0.366385  1.219771  3.181852  1.968928  0.897587   \n",
       "3  0.631253  0.769837  0.853429  2.068941  4.745625  3.921846  8.303919   \n",
       "4  0.132524  0.325776  0.527773  1.888710  4.970187  0.824709  3.522625   \n",
       "\n",
       "      rmse_5    rmse_21     rmse_63  \n",
       "0   5.298430  19.232196  152.996531  \n",
       "1   6.716697  28.399187  198.357708  \n",
       "2   3.338171  34.673138  164.556947  \n",
       "3  11.903305  60.231971  245.860423  \n",
       "4   7.398902  55.093640  257.900757  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mape_1</th>\n      <th>mape_3</th>\n      <th>mape_5</th>\n      <th>mape_21</th>\n      <th>mape_63</th>\n      <th>rmse_1</th>\n      <th>rmse_3</th>\n      <th>rmse_5</th>\n      <th>rmse_21</th>\n      <th>rmse_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.174163</td>\n      <td>0.548679</td>\n      <td>0.452738</td>\n      <td>0.878127</td>\n      <td>3.019335</td>\n      <td>1.084636</td>\n      <td>4.623117</td>\n      <td>5.298430</td>\n      <td>19.232196</td>\n      <td>152.996531</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.696150</td>\n      <td>0.701604</td>\n      <td>0.485295</td>\n      <td>1.201450</td>\n      <td>3.898030</td>\n      <td>4.307080</td>\n      <td>7.515089</td>\n      <td>6.716697</td>\n      <td>28.399187</td>\n      <td>198.357708</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.319051</td>\n      <td>0.295229</td>\n      <td>0.366385</td>\n      <td>1.219771</td>\n      <td>3.181852</td>\n      <td>1.968928</td>\n      <td>0.897587</td>\n      <td>3.338171</td>\n      <td>34.673138</td>\n      <td>164.556947</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.631253</td>\n      <td>0.769837</td>\n      <td>0.853429</td>\n      <td>2.068941</td>\n      <td>4.745625</td>\n      <td>3.921846</td>\n      <td>8.303919</td>\n      <td>11.903305</td>\n      <td>60.231971</td>\n      <td>245.860423</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.132524</td>\n      <td>0.325776</td>\n      <td>0.527773</td>\n      <td>1.888710</td>\n      <td>4.970187</td>\n      <td>0.824709</td>\n      <td>3.522625</td>\n      <td>7.398902</td>\n      <td>55.093640</td>\n      <td>257.900757</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "def evaluate(df, n_periods):\n",
    "    n_periods = 62 if n_periods == 63 else n_periods\n",
    "    df = df[-63:-63+n_periods]\n",
    "    mape = ((df[\"abs_error\"] / df[\"Close\"]).sum() / n_periods) * 100\n",
    "    rmse = math.sqrt(pow(df[\"error\"].sum(), 2) / n_periods)\n",
    "    return mape, rmse\n",
    "\n",
    "cross_df = pd.DataFrame(columns=[\n",
    "    \"mape_1\", \n",
    "    \"mape_3\",\n",
    "    \"mape_5\",\n",
    "    \"mape_21\",\n",
    "    \"mape_63\",\n",
    "    \"rmse_1\",\n",
    "    \"rmse_3\",\n",
    "    \"rmse_5\",\n",
    "    \"rmse_21\",\n",
    "    \"rmse_63\"\n",
    "])\n",
    "\n",
    "for i in range(len(forecasts)):\n",
    "    train_df = original_df[:-n_test + i].copy()\n",
    "    train_df[\"forecast\"] = train_df[\"Close\"]\n",
    "\n",
    "    last_train = train_df[\"Close\"].values[-1]\n",
    "    price_forecasts = np.exp(np.cumsum(forecasts[i]) + math.log(last_train))\n",
    "\n",
    "    merged_df = original_df[-n_test + i:-n_test + i + 63].copy()\n",
    "    merged_df[\"forecast\"] = price_forecasts\n",
    "\n",
    "    merged_df[\"error\"] = merged_df[\"forecast\"] - merged_df[\"Close\"]\n",
    "    merged_df[\"abs_error\"] = np.abs(merged_df[\"forecast\"] - merged_df[\"Close\"])\n",
    "\n",
    "    one = evaluate(merged_df, 1)\n",
    "    three = evaluate(merged_df, 3)\n",
    "    five = evaluate(merged_df, 5)\n",
    "    twentyone = evaluate(merged_df, 21)\n",
    "    sixtythree = evaluate(merged_df, 63)\n",
    "\n",
    "    cross_df = cross_df.append({\n",
    "        'mape_1': one[0],\n",
    "        'mape_3': three[0],\n",
    "        'mape_5': five[0],\n",
    "        'mape_21': twentyone[0],\n",
    "        'mape_63': sixtythree[0],\n",
    "        'rmse_1': one[1],\n",
    "        'rmse_3': three[1],\n",
    "        'rmse_5': five[1],\n",
    "        'rmse_21': twentyone[1],\n",
    "        'rmse_63': sixtythree[1],\n",
    "    }, ignore_index=True)\n",
    "\n",
    "cross_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "           mape_1      mape_3      mape_5     mape_21     mape_63      rmse_1  \\\n",
       "count  100.000000  100.000000  100.000000  100.000000  100.000000  100.000000   \n",
       "mean     0.452493    0.675881    0.830413    1.780169    5.180900    2.990469   \n",
       "std      0.379413    0.455349    0.510669    0.828867    1.651811    2.508202   \n",
       "min      0.001283    0.082064    0.209006    0.546482    1.286949    0.008662   \n",
       "25%      0.173810    0.338734    0.443037    1.188126    4.119238    1.133718   \n",
       "50%      0.349202    0.613664    0.727641    1.639635    5.185377    2.301006   \n",
       "75%      0.632679    0.860807    1.176574    2.150871    6.289932    4.002759   \n",
       "max      1.911459    2.927850    3.177644    5.041456    8.837445   12.921274   \n",
       "\n",
       "           rmse_3      rmse_5     rmse_21     rmse_63  \n",
       "count  100.000000  100.000000  100.000000  100.000000  \n",
       "mean     7.125819   10.835464   49.289116  285.139219  \n",
       "std      5.560760    8.493985   30.302296  110.592964  \n",
       "min      0.163984    0.012091    0.303191    3.898016  \n",
       "25%      2.961218    3.540514   27.662235  210.652777  \n",
       "50%      6.265102    8.823601   45.244415  278.343059  \n",
       "75%      9.965476   17.338111   64.117900  365.430380  \n",
       "max     34.140031   47.918366  158.952991  503.228713  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mape_1</th>\n      <th>mape_3</th>\n      <th>mape_5</th>\n      <th>mape_21</th>\n      <th>mape_63</th>\n      <th>rmse_1</th>\n      <th>rmse_3</th>\n      <th>rmse_5</th>\n      <th>rmse_21</th>\n      <th>rmse_63</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n      <td>100.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>0.452493</td>\n      <td>0.675881</td>\n      <td>0.830413</td>\n      <td>1.780169</td>\n      <td>5.180900</td>\n      <td>2.990469</td>\n      <td>7.125819</td>\n      <td>10.835464</td>\n      <td>49.289116</td>\n      <td>285.139219</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.379413</td>\n      <td>0.455349</td>\n      <td>0.510669</td>\n      <td>0.828867</td>\n      <td>1.651811</td>\n      <td>2.508202</td>\n      <td>5.560760</td>\n      <td>8.493985</td>\n      <td>30.302296</td>\n      <td>110.592964</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.001283</td>\n      <td>0.082064</td>\n      <td>0.209006</td>\n      <td>0.546482</td>\n      <td>1.286949</td>\n      <td>0.008662</td>\n      <td>0.163984</td>\n      <td>0.012091</td>\n      <td>0.303191</td>\n      <td>3.898016</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>0.173810</td>\n      <td>0.338734</td>\n      <td>0.443037</td>\n      <td>1.188126</td>\n      <td>4.119238</td>\n      <td>1.133718</td>\n      <td>2.961218</td>\n      <td>3.540514</td>\n      <td>27.662235</td>\n      <td>210.652777</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>0.349202</td>\n      <td>0.613664</td>\n      <td>0.727641</td>\n      <td>1.639635</td>\n      <td>5.185377</td>\n      <td>2.301006</td>\n      <td>6.265102</td>\n      <td>8.823601</td>\n      <td>45.244415</td>\n      <td>278.343059</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>0.632679</td>\n      <td>0.860807</td>\n      <td>1.176574</td>\n      <td>2.150871</td>\n      <td>6.289932</td>\n      <td>4.002759</td>\n      <td>9.965476</td>\n      <td>17.338111</td>\n      <td>64.117900</td>\n      <td>365.430380</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>1.911459</td>\n      <td>2.927850</td>\n      <td>3.177644</td>\n      <td>5.041456</td>\n      <td>8.837445</td>\n      <td>12.921274</td>\n      <td>34.140031</td>\n      <td>47.918366</td>\n      <td>158.952991</td>\n      <td>503.228713</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "cross_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}